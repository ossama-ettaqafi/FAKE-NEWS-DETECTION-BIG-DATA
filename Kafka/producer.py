import pandas as pd
from kafka import KafkaProducer
import time
import json

# Configuration Kafka
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # JSON propre
)

topic = 'news-topic'
file_path = 'final_news.tsv'  # ğŸŸ¢ Nouveau fichier TSV

# Fonction d'envoi d'un chunk Ã  Kafka
def send_to_kafka(chunk):
    for _, row in chunk.iterrows():
        try:
            if pd.notna(row['text']) and pd.notna(row['label']):
                message = {'text': row['text'], 'label': row['label']}
                producer.send(topic, value=message)
                print(f"âœ… EnvoyÃ© : {message}")
                time.sleep(0.1)  # ğŸ• Petit dÃ©lai entre les envois (accÃ©lÃ©rÃ©)
            else:
                print("âš ï¸ Ligne ignorÃ©e (valeurs manquantes).")
        except Exception as e:
            print(f"âŒ Erreur ligne : {e}")

try:
    # ğŸŒ€ Lire et mÃ©langer toutes les donnÃ©es
    data = pd.read_csv(file_path, sep='\t')
    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle complet ğŸ”¥

    # ğŸ§¹ Envoyer par chunks
    chunk_size = 100
    for i in range(0, len(data), chunk_size):
        chunk = data.iloc[i:i+chunk_size]
        print(f"ğŸ“¦ Chunk chargÃ© ({chunk.shape[0]} lignes) : colonnes = {chunk.columns.tolist()}")
        send_to_kafka(chunk)

except Exception as e:
    print(f"âŒ Erreur de lecture du fichier : {e}")

# Fermeture propre
producer.flush()
producer.close()
