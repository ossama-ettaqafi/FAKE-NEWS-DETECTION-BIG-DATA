# ğŸ“° Fake News Detection â€” Big Data Pipeline

A real-time pipeline for detecting fake news using Apache Kafka, Spark, Cassandra, and a Flask-based dashboard. The system streams articles, applies pre-trained ML models for classification, and stores results for visualization.

## ğŸ“ Project Structure

```

FakeNewsDetectionBigData/
â”œâ”€â”€ producer.py                 # Kafka producer: streams dataset to topic
â”œâ”€â”€ consumer.py                 # Spark job: consumes, predicts, stores to Cassandra
â”œâ”€â”€ dashboard.py                # Flask dashboard for live prediction view
â”œâ”€â”€ models/                     # Pretrained models (Naive Bayes, SVM, TF-IDF)
â”œâ”€â”€ data/                       # Streaming dataset (.tsv)
â”œâ”€â”€ config/                     # Contains settings.py (Kafka config, paths)
â”œâ”€â”€ templates/                  # HTML template for dashboard
â”œâ”€â”€ notebooks/                  # Jupyter notebook for training
â”‚   â””â”€â”€ FakeNewsDetection_ML.ipynb
â”œâ”€â”€ scripts/                    # Batch launcher
â”‚   â””â”€â”€ run_all.bat
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md

````

## âš™ Technologies Used

| Component          | Tool / Library                      |
|--------------------|-------------------------------------|
| Data Streaming     | Apache Kafka                        |
| Stream Processing  | Apache Spark                        |
| Machine Learning   | Python, Scikit-learn                |
| Models             | Naive Bayes, SVM (TF-IDF features)  |
| Storage            | Apache Cassandra (NoSQL)            |
| Dashboard          | Flask + HTML                        |
| Dataset            | TSV format with `text` and `label`  |

---

## ğŸš€ How to Run the Pipeline

### Option 1: Use the automated script

From the root of the project, run:

```bash
scripts\run_all.bat
````

This script will:

1. Start Zookeeper
2. Start Kafka
3. Create the Kafka topic `news`
4. Start the Kafka producer
5. Start the Spark consumer
6. Launch the Flask dashboard

Each step includes delays to ensure services initialize properly.

### Option 2: Manual Startup (Step-by-step)

#### 1. Start Zookeeper

```bash
D:\kafka_2.12-2.5.0\bin\windows\zookeeper-server-start.bat D:\kafka_2.12-2.5.0\config\zookeeper.properties
```

#### 2. Start Kafka

```bash
D:\kafka_2.12-2.5.0\bin\windows\kafka-server-start.bat D:\kafka_2.12-2.5.0\config\server.properties
```

#### 3. Create the Kafka topic

```bash
D:\kafka_2.12-2.5.0\bin\windows\kafka-topics.bat --create ^
  --topic news ^
  --bootstrap-server localhost:9092 ^
  --partitions 1 ^
  --replication-factor 1
```

#### 4. Start the Kafka producer

```bash
python producer.py
```

#### 5. Start the Spark consumer

```bash
set PYSPARK_PYTHON=python
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 consumer.py
```

#### 6. Start the Flask dashboard

```bash
python dashboard.py
```

## ğŸ§ª Model Training

* Training is done in `notebooks/FakeNewsDetection_ML.ipynb`
* Models are saved as `.pkl` files in the `models/` directory
* Used classifiers: Naive Bayes and SVM with TF-IDF vectorization

## ğŸ—ƒ Cassandra Setup

Launch `cqlsh` and run:

```sql
CREATE KEYSPACE IF NOT EXISTS fakenews
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

USE fakenews;

CREATE TABLE IF NOT EXISTS predictions_streaming (
    id UUID PRIMARY KEY,
    text_short TEXT,
    label INT,
    prediction DOUBLE,
    model TEXT
);

CREATE TABLE IF NOT EXISTS evaluation_streaming (
    batch_id BIGINT,
    model TEXT,
    timestamp TEXT,
    accuracy DOUBLE,
    precision DOUBLE,
    recall DOUBLE,
    f1_score DOUBLE,
    PRIMARY KEY (batch_id, model)
);
```

## ğŸ”§ Configuration

Edit `config/settings.py` to change:

```python
# Kafka Configuration
KAFKA_CONFIG = {
    'KAFKA_BROKER': 'localhost:9092',
    'KAFKA_TOPIC': 'news'
}

# File path for data
DATA_FILE_PATH = 'data/final_fake_real_news.tsv'

# Model file paths
MODEL_PATHS = {
    'tfidf': 'models/tfidf_vectorizer.pkl',
    'naive_bayes': 'models/naive_bayes_model.pkl',
    'svm': 'models/svm_model.pkl'
}

# Cassandra configuration
CASSANDRA_CONFIG = {
    'host': '127.0.0.1',
    'keyspace': 'fakenews'
}
```

## ğŸ“‹ Requirements

Install required packages:

```bash
pip install -r requirements.txt
```

## ğŸ“Œ Notes

* Kafka 2.5.0, Spark 3.4.x, Cassandra 3.11.x recommended
* Cassandra `cqlsh` may require Python 2.7
* Kafka/Zookeeper logs and metadata may live under `C:\tmp\` unless redirected

## ğŸ“„ License

MIT License
